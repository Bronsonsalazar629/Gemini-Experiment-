{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Fairness Analysis Demo\n",
    "\n",
    "This notebook demonstrates the Obermeyer kidney algorithm bias case and how to detect and remediate it using the Clinical Fairness Intervention System.\n",
    "\n",
    "## Background\n",
    "\n",
    "The Obermeyer et al. (2019) study revealed significant racial bias in a widely-used healthcare algorithm for chronic kidney disease management. The algorithm systematically underestimated disease severity for Black patients, leading to reduced access to specialist care.\n",
    "\n",
    "**Key findings:**\n",
    "- The algorithm used healthcare costs as a proxy for health needs\n",
    "- Black patients had lower healthcare costs due to reduced access to care\n",
    "- This created a feedback loop where bias in the system reinforced bias in the algorithm\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Reproduce the bias pattern using synthetic clinical data\n",
    "2. Use causal analysis to identify bias pathways\n",
    "3. Detect fairness violations using multiple metrics\n",
    "4. Generate and apply fairness interventions\n",
    "5. Evaluate the effectiveness of interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Import our modules\n",
    "from causal_analysis import CausalAnalyzer\n",
    "from bias_detection import BiasDetector\n",
    "from intervention_engine import InterventionEngine\n",
    "from code_generator import CodeGenerator\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic clinical data\n",
    "data_path = Path.cwd().parent / \"data\" / \"sample\" / \"demo_data.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Loaded {len(data)} patient records\")\n",
    "print(f\"\\nColumns: {list(data.columns)}\")\n",
    "print(f\"\\nData shape: {data.shape}\")\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data distribution\n",
    "print(\"\\n=== Data Summary ===\")\n",
    "print(f\"\\nRace distribution:\")\n",
    "print(data['race'].value_counts())\n",
    "\n",
    "print(f\"\\nGender distribution:\")\n",
    "print(data['gender'].value_counts())\n",
    "\n",
    "print(f\"\\nReferral rate: {data['referral'].mean():.2%}\")\n",
    "\n",
    "print(f\"\\nReferral rate by race:\")\n",
    "print(data.groupby('race')['referral'].agg(['mean', 'count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outcome distribution by protected attribute\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Referral rate by race\n",
    "data.groupby('race')['referral'].mean().plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Referral Rate by Race', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Referral Rate')\n",
    "axes[0].set_xlabel('Race')\n",
    "axes[0].axhline(y=data['referral'].mean(), color='red', linestyle='--', label='Overall Mean')\n",
    "axes[0].legend()\n",
    "\n",
    "# Creatinine levels by race and referral\n",
    "data.boxplot(column='creatinine_level', by=['race', 'referral'], ax=axes[1])\n",
    "axes[1].set_title('Creatinine Levels by Race and Referral Status', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Race, Referral')\n",
    "axes[1].set_ylabel('Creatinine Level')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInitial observation: Disparate referral rates across racial groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a Biased Model (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "data_encoded = data.copy()\n",
    "le_race = LabelEncoder()\n",
    "le_gender = LabelEncoder()\n",
    "le_insurance = LabelEncoder()\n",
    "\n",
    "data_encoded['race_encoded'] = le_race.fit_transform(data['race'])\n",
    "data_encoded['gender_encoded'] = le_gender.fit_transform(data['gender'])\n",
    "data_encoded['insurance_encoded'] = le_insurance.fit_transform(data['insurance_type'])\n",
    "\n",
    "print(\"Race encoding:\", dict(enumerate(le_race.classes_)))\n",
    "\n",
    "# Prepare features\n",
    "feature_cols = [\n",
    "    'age', 'race_encoded', 'gender_encoded', 'creatinine_level',\n",
    "    'chronic_conditions', 'insurance_encoded', 'prior_visits',\n",
    "    'distance_to_hospital'\n",
    "]\n",
    "\n",
    "X = data_encoded[feature_cols + ['race']]  # Keep race for fairness analysis\n",
    "y = data_encoded['referral']\n",
    "\n",
    "# Train-test split (stratified by race)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=data_encoded['race']\n",
    ")\n",
    "\n",
    "# Train baseline model\n",
    "baseline_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "baseline_model.fit(X_train[feature_cols], y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_baseline = baseline_model.predict(X_test[feature_cols])\n",
    "accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(f\"\\nBaseline Model Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'coefficient': baseline_model.coef_[0]\n",
    "}).sort_values('coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (Coefficients):\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Causal Analysis - Identify Bias Pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize causal analyzer\n",
    "analyzer = CausalAnalyzer(data, 'race', 'referral')\n",
    "\n",
    "# Infer causal graph\n",
    "graph, explanation = analyzer.infer_causal_graph(llm_enhanced=False)\n",
    "\n",
    "print(\"\\n=== CAUSAL GRAPH ANALYSIS ===\")\n",
    "print(explanation)\n",
    "\n",
    "# Identify bias pathways\n",
    "pathways = analyzer.identify_bias_pathways()\n",
    "\n",
    "print(\"\\n=== BIAS PATHWAYS ===\")\n",
    "print(f\"Found {len(pathways)} causal pathways from race to referral:\\n\")\n",
    "for i, path in enumerate(pathways, 1):\n",
    "    print(f\"{i}. {' -> '.join(path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bias Detection - Quantify Fairness Violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize bias detector\n",
    "detector = BiasDetector(['race'])\n",
    "\n",
    "# Compute fairness metrics\n",
    "metrics = detector.compute_fairness_metrics(\n",
    "    baseline_model, X_test, y_test, 'race'\n",
    ")\n",
    "\n",
    "# Generate report\n",
    "report = detector.generate_bias_report(metrics, 'race')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fairness metrics\n",
    "group_metrics = metrics['group_metrics']\n",
    "groups = list(group_metrics.keys())\n",
    "\n",
    "# Prepare data for visualization\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Group': groups,\n",
    "    'Positive Rate': [group_metrics[g]['positive_rate'] for g in groups],\n",
    "    'TPR (Recall)': [group_metrics[g]['true_positive_rate'] for g in groups],\n",
    "    'FPR': [group_metrics[g]['false_positive_rate'] for g in groups],\n",
    "    'Accuracy': [group_metrics[g]['accuracy'] for g in groups]\n",
    "})\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Positive Rate\n",
    "axes[0, 0].bar(metrics_df['Group'], metrics_df['Positive Rate'], color='steelblue')\n",
    "axes[0, 0].set_title('Positive Prediction Rate by Group', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Rate')\n",
    "axes[0, 0].axhline(y=metrics_df['Positive Rate'].mean(), color='red', linestyle='--')\n",
    "\n",
    "# TPR vs FPR\n",
    "x = np.arange(len(groups))\n",
    "width = 0.35\n",
    "axes[0, 1].bar(x - width/2, metrics_df['TPR (Recall)'], width, label='TPR', color='green')\n",
    "axes[0, 1].bar(x + width/2, metrics_df['FPR'], width, label='FPR', color='red')\n",
    "axes[0, 1].set_title('TPR vs FPR by Group', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Rate')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(groups)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Accuracy\n",
    "axes[1, 0].bar(metrics_df['Group'], metrics_df['Accuracy'], color='purple')\n",
    "axes[1, 0].set_title('Accuracy by Group', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].axhline(y=metrics_df['Accuracy'].mean(), color='red', linestyle='--')\n",
    "\n",
    "# Fairness criteria summary\n",
    "fairness_data = {\n",
    "    'Metric': ['Demographic\\nParity', 'Equalized\\nOdds', 'Equal\\nOpportunity'],\n",
    "    'Difference': [\n",
    "        metrics['demographic_parity']['difference'],\n",
    "        metrics['equalized_odds']['average_difference'],\n",
    "        metrics['equal_opportunity']['difference']\n",
    "    ]\n",
    "}\n",
    "colors = ['red' if d > 0.1 else 'green' for d in fairness_data['Difference']]\n",
    "axes[1, 1].bar(fairness_data['Metric'], fairness_data['Difference'], color=colors)\n",
    "axes[1, 1].set_title('Fairness Criteria Violations', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Difference')\n",
    "axes[1, 1].axhline(y=0.1, color='orange', linestyle='--', label='Threshold (0.1)')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Intervention Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get intervention recommendations\n",
    "engine = InterventionEngine(bias_threshold=0.1)\n",
    "recommendations = engine.suggest_interventions(\n",
    "    metrics,\n",
    "    max_recommendations=5,\n",
    "    prioritize_accuracy=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== INTERVENTION RECOMMENDATIONS ===\")\n",
    "print(f\"\\nGenerated {len(recommendations)} recommendations:\\n\")\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"\\n{rec.priority}. {rec.name} ({rec.category})\")\n",
    "    print(f\"   Description: {rec.description}\")\n",
    "    print(f\"   Expected Impact: {rec.expected_impact}\")\n",
    "    print(f\"   Complexity: {rec.complexity}\")\n",
    "    print(f\"   Preserves Accuracy: {rec.preserves_accuracy}\")\n",
    "    print(f\"   Parameters: {rec.parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Intervention Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate code for top recommendation\n",
    "if recommendations:\n",
    "    top_intervention = recommendations[0]\n",
    "    \n",
    "    print(f\"\\nGenerating code for: {top_intervention.name}\")\n",
    "    \n",
    "    generator = CodeGenerator()\n",
    "    code_result = generator.generate_fix_code(top_intervention.name)\n",
    "    \n",
    "    print(\"\\n=== GENERATED CODE ===\")\n",
    "    print(\"\\nImports:\")\n",
    "    for imp in code_result.imports:\n",
    "        print(f\"  {imp}\")\n",
    "    \n",
    "    print(\"\\nCode:\")\n",
    "    print(code_result.code[:1000] + \"...\" if len(code_result.code) > 1000 else code_result.code)\n",
    "    \n",
    "    print(\"\\nUsage Example:\")\n",
    "    print(code_result.usage_example[:500] + \"...\" if len(code_result.usage_example) > 500 else code_result.usage_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "### Findings:\n",
    "1. **Causal Pathways**: Identified direct and indirect bias pathways from race to referral decisions\n",
    "2. **Fairness Violations**: Detected violations in demographic parity, equalized odds, and equal opportunity\n",
    "3. **Intervention Strategy**: Generated actionable recommendations to mitigate bias\n",
    "\n",
    "### Recommendations:\n",
    "- Apply preprocessing interventions (reweighing/resampling) to balance training data\n",
    "- Consider postprocessing methods to adjust predictions for fairness\n",
    "- Monitor fairness metrics continuously in production\n",
    "- Combine technical interventions with policy changes\n",
    "\n",
    "### Next Steps:\n",
    "1. Implement the recommended intervention\n",
    "2. Re-evaluate fairness metrics\n",
    "3. Assess fairness-accuracy tradeoffs\n",
    "4. Deploy with ongoing monitoring\n",
    "\n",
    "### References:\n",
    "- Obermeyer, Z., et al. (2019). \"Dissecting racial bias in an algorithm used to manage the health of populations.\" Science, 366(6464), 447-453.\n",
    "- AIF360: IBM's AI Fairness 360 Toolkit\n",
    "- Fairlearn: Microsoft's Fairness Assessment and Improvement Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for further analysis\n",
    "print(\"\\nNotebook execution complete!\")\n",
    "print(\"\\nTo continue:\")\n",
    "print(\"1. Implement the recommended intervention\")\n",
    "print(\"2. Use the generated code as a starting point\")\n",
    "print(\"3. Re-run bias detection to validate improvements\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
